{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Agent\n",
    "\n",
    "Learns:\n",
    "- https://codelabs.developers.google.com/codelabs/gemini-function-calling#3\n",
    "- done: https://python.langchain.com/docs/modules/agents/how_to/custom_agent\n",
    "- done: https://python.langchain.com/docs/modules/agents/agent_types/structured_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h4438/miniconda3/envs/uni/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from src.config import Configuration\n",
    "\n",
    "conf = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.set_tavily_token()\n",
    "tavily = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://python.langchain.com/docs/get_started/introduction',\n",
       "  'content': \"Modules\\u200b\\nLangChain provides standard, extendable interfaces and integrations for the following modules:\\nInterface with language models\\nInterface with application-specific data\\nLet models choose which tools to use given high-level directives\\nExamples, ecosystem, and resources\\u200b\\nUse cases\\u200b\\nWalkthroughs and techniques for common end-to-end use cases, like:\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Together, these products simplify the entire application lifecycle:\\nLangChain Libraries\\u200b\\nThe main value props of the LangChain packages are:\\nOff-the-shelf chains make it easy to get started. API reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.\\n Developer's guide\\u200b\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\\n LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€ chain to the most complex chains.\\n\"},\n",
       " {'url': 'https://www.ibm.com/topics/langchain',\n",
       "  'content': 'Applications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a â€œreasoning engine.â€\\nTrain, validate, tune and deploy generative AI, foundation models and machine learning capabilities with ease and build AI applications in a fraction of the time with a fraction of the data.\\n When building a chain for an agent, inputs include:\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0(link resides outside ibm.com) are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Launched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAIâ€™s ChatGPT the following month, LangChain has played a significant role in making generative AI more accessible to enthusiasts in the wake of its widespread popularity.\\n Reimagine how you work with\\xa0AI: our diverse, global team of more than 20,000 AI\\xa0experts can help you quickly and confidently\\xa0design and scale AI and automation across your\\xa0business, working across our own IBM\\xa0watsonx\\xa0technology\\xa0and an open ecosystem of partners to deliver any\\xa0AI model, on any cloud, guided by ethics and trust.\\n While itâ€™s the GPT model that interprets the userâ€™s input and composes a natural language response, itâ€™s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience.'},\n",
       " {'url': 'https://www.techtarget.com/searchEnterpriseAI/definition/LangChain',\n",
       "  'content': \"Everything you need to know\\nWhat are the features of LangChain?\\nLangChain is made up of the following modules that ensure the multiple components needed to make an effective NLP app can run smoothly:\\nWhat are the integrations of LangChain?\\nLangChain typically builds applications using integrations with LLM providers and external sources where data can be found and stored. What is synthetic data?\\nExamples and use cases for LangChain\\nThe LLM-based applications LangChain is capable of building can be applied to multiple advanced use cases within various industries and vertical markets, such as the following:\\nReaping the benefits of NLP is a key of why LangChain is important. As the airline giant moves more of its data workloads to the cloud, tools from Intel's Granulate are making platforms such as ...\\nThe vendor's new platform, now in beta testing, combines its existing lakehouse with AI to better enable users to manage and ...\\n The following steps are required to use this:\\nIn this scenario, the language model would be expected to take the two input variables -- the adjective and the content -- and produce a fascinating fact about zebras as its output.\\n The goal of LangChain is to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications.\\n\"},\n",
       " {'url': 'https://en.wikipedia.org/wiki/LangChain',\n",
       "  'content': 'In October 2023 LangChain introduced LangServe, a deployment tool designed to facilitate the transition from LCEL (LangChain Expression Language) prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database[6] to store and retrieve vector embeddings; Weaviate vector database[7] to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project\\'s Discord server, many YouTube tutorials, and meetups in San Francisco and London. As of April 2023, it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal links[edit]'},\n",
       " {'url': 'https://aws.amazon.com/what-is/langchain/',\n",
       "  'content': 'Overview\\nTo use LangChain, developers install the framework in Python with the following command:\\npip install langchain\\nDevelopers then use the chain building blocks or LangChain Expression Language (LCEL) to compose chains with simple programming commands. To do that, machine learning engineers must integrate the LLM with the organizationâ€™s internal data sources and apply prompt engineeringâ€”a practice where a data scientist refines inputs to a generative model with a specific structure and context.\\n How does LangChain work?\\nWith LangChain, developers can adapt a language model flexibly to specific business contexts by designating steps required to produce the desired outcome.\\n Examples of links include:\\nIn the LangChain framework, a link accepts input from the user and passes it to the LangChain libraries for processing. What are the core components of LangChain?\\nUsing LangChain, software teams can build context-aware language model systems with the following modules.\\n'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tavily.invoke(input=\"what is langchain\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, conf.get_gemini_embeddings())\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = retriever.get_relevant_documents(\"how to upload a dataset\")[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "\n",
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "get_word_length.invoke(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "tools = [tavily, retriever_tool, get_word_length]\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = conf.get_gemini_pro(True)\n",
    "\n",
    "# Construct the JSON agent\n",
    "agent = create_structured_chat_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "conf.enable_tracing(project=\"LEARN\")\n",
    "\n",
    "@traceable(tags=[\"agent\"])\n",
    "def run_agent(q:str):\n",
    "    a = agent_executor.invoke({\"input\": q})\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"get_word_length\",\n",
      "  \"action_input\": {\n",
      "    \"word\": \"hello world\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\u001b[38;5;200m\u001b[1;3m11\u001b[0m\u001b[32;1m\u001b[1;3m```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Eleven\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"how many characters in 'hello world'\", 'output': 'Eleven'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"how many characters in 'hello world'\"\n",
    "a = run_agent(q)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
